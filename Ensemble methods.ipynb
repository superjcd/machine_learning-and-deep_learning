{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods--Bagging & Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  在上一篇[Full_Project_Process](https://nbviewer.jupyter.org/github/superjcd/ML-DL/blob/master/Full_Project_Process.ipynb#)里我们为了预测房价，训练了几种不同的模型,最终表现最好（依据cross_val_score）的模型是alpha值为0.1的lasso回归，其平均交叉验证误差为69052.4。这一回，我将使用一种machine learning领域非常强有力的训练方法（Essemble methods）来改进模型的预测效果。essemble models的基本思想就是与其训练一个好的模型，不如训练若干个表现一般的模型然后将它们的结果ensemble起来（对于回归问题，比如各个模型预测值的平均），实践证明essemble models通常具有比一般模型更好的预测效果，也是这几年kaggle比赛中表现最好的两种主流方法之一（另一种是deep learnng）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ML_tools 汇集了上一篇中所用过的若干类\n",
    "from ML_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#采取和上次一样的数据分割方法\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "House = pd.read_csv('Data/housing.csv')\n",
    "House[\"income_cat\"] = np.ceil(House[\"median_income\"] / 1.5)\n",
    "# Label those above 5 as 5\n",
    "House[\"income_cat\"].where(House[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "strait = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) #strait split提供n对保留原有数据结构的train，test index\n",
    "for train_index, test_index in strait.split(House, House[\"income_cat\"]): #.split(X,y)\n",
    "    strat_train_set = House.loc[train_index]\n",
    "    strat_test_set = House.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_x = strat_train_set.drop(['median_house_value'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_y = strat_train_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512, 10), (16512,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_x.shape,Train_y.shape #和之前是一模一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "num_attribs = Train_x.columns[:-2]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([   #list of tuples,the easy way is make_pipeline\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "Train_x= full_pipeline.fit_transform(Train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Bagging --a sample of decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  模型因为会尽可能”记住“训练数据，所以往往在新数据上过拟合。为了减少由于variance带来的error(error = bias+variance)，我们可以在模型训练的时候对数据加入variance，这样模型就只能‘记住’部分而不是全部，这样就可以减少test error。对于回归问题，我们只要对各个由随机数据集获得的模型的预测值进行平均(分类问题使用投票)，就能得到最终的预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "bg_reg = BaggingRegressor(DecisionTreeRegressor(random_state=42),\n",
    "                         n_estimators=100, #训练100个模型\n",
    "                          max_samples=200,  #每次选取200个数据\n",
    "                          bootstrap=True,  #使用bootsrap方法，即放回式sample，允许重复\n",
    "                         n_jobs =-1,   #使用尽可能多的cpu\n",
    "                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60675.64829676476"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(bg_reg,Train_x,Train_y,cv=5,scoring='neg_mean_squared_error')\n",
    "cvscore_bg_tree = np.sqrt(-score).mean()\n",
    "cvscore_bg_tree #通过bagging改进的决策树模型要远远低于原有的误差,值得注意的是如果是BaggingCalssier的话，模型自带obb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 一般的bagging方法，像上面提到的这种，是对数据进行随机选取自己，然后进行训练。随机森林，在此基础上，对变量也进行了一次随机选取，这样\n",
    "可以极大地提升模型在验证集上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(n_estimators=200,\n",
    "                               n_jobs=-1, \n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = cross_val_score(rf_reg,Train_x,Train_y,scoring='neg_mean_squared_error',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50273.665635143254"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscore_rf = np.sqrt(-score).mean()\n",
    "cvscore_rf #我们选取的参数获得较好的预测效果，但是是不是能更近一步呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "           verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1147abe10>, 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1158d06a0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_squared_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fine tune the model\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=10),\n",
    "    }#如果我们使用grid_search的话将会由200*10=2000种可能性\n",
    "\n",
    "rnd_search = RandomizedSearchCV(rf_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)#多了n_iter参数\n",
    "\n",
    "rnd_search.fit(Train_x,Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49147.15241724505 {'max_features': 7, 'n_estimators': 180}\n",
      "49054.217812866234 {'max_features': 8, 'n_estimators': 189}\n",
      "49581.75837312377 {'max_features': 5, 'n_estimators': 103}\n",
      "50776.736049370644 {'max_features': 3, 'n_estimators': 75}\n",
      "49142.307571955236 {'max_features': 8, 'n_estimators': 117}\n",
      "49621.00032993271 {'max_features': 4, 'n_estimators': 104}\n",
      "49133.89163784365 {'max_features': 8, 'n_estimators': 131}\n",
      "49649.95109531419 {'max_features': 6, 'n_estimators': 53}\n",
      "52008.30195205342 {'max_features': 2, 'n_estimators': 88}\n",
      "49330.20930219184 {'max_features': 6, 'n_estimators': 130}\n"
     ]
    }
   ],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)#可以看到最小的error为49147要远远小于前面的所有方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rf_model=rnd_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#验证最好的模型在test set上的表现\n",
    "Test_x = strat_test_set.drop(['median_house_value'],axis=1)\n",
    "Test_y = strat_test_set['median_house_value']\n",
    "\n",
    "Test_x = full_pipeline.fit_transform(Test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=8, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=189, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model.fit(Train_x,Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49795.67310738895"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "rf_predictions = best_rf_model.predict(Test_x)\n",
    "\n",
    "#mean_absolute_error\n",
    "mean_absolute_error(Test_y,rf_predictions) #误差不到5万"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importance \n",
    "np.argmax(best_rf_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'median_income'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "House.columns[7] #meadian_income 做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=1),n_estimators=200, learning_rate=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94855.57241601839"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(ada_reg,Train_x,Train_y,cv=5,scoring='neg_mean_squared_error')\n",
    "\n",
    "cvscore_ada_tree = np.sqrt(-score).mean()\n",
    "cvscore_ada_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Gradiant Boosting\n",
    "  Gradiant的基本原理是这样的，第一步，用简单的模型（比如线形模型和深度较浅的回归树模型）在Train_set上训练，然后对上一步的误差进行进一步的\n",
    " 训练，然后重复这个过程。在预测的时候只要把各个模型的预测值加起来即可，如下下图所示（来源：Hands_on_machine_learning）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![示意图](./fig/GradiantBoost.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                n_estimators=100,\n",
    "                                max_depth=3,\n",
    "                                min_samples_split=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=3,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.fit(Train_x,Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_gbrt = gbrt.predict(Train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35102.22759200338"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error_gbrt = mean_absolute_error(Train_y,predictions_gbrt)\n",
    "train_error_gbrt #train error is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross val score\n",
    "score = cross_val_score(gbrt,Train_x,Train_y,cv=5,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53276.77228589257"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscore_gbrt = np.sqrt(-score).mean()\n",
    "cvscore_gbrt #略高于前面的随机森里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何更进一步地提升模型？那么就需要改变我们的gradiantboostingregressor 的参数。 \n",
    "方法有以下三种：   \n",
    "1、Gridsearch cross validation   \n",
    "2、Randomsearch cross validaton   \n",
    "3、early-stop（可以给予不同的指标，比如mse是不是持续下降，当然也可以结合AIC，BIC的指标）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这里我们采用一种结合random search 和 grid search的方法\n",
    "gbrt = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "param_distribs = {\n",
    "        'learning_rate':[0.1,0.3,0.5,1],\n",
    "        'n_estimators':randint(low=40, high=300),\n",
    "        'max_features': randint(low=1, high=10),\n",
    "        'max_depth': randint(low=1, high=10),\n",
    "        'min_samples_leaf':randint(low=1,high=10)\n",
    "    }\n",
    "\n",
    "rsc_gbtr=RandomizedSearchCV(gbrt,param_distributions=param_distribs,\n",
    "                                n_iter=30, cv=5, scoring='neg_mean_squared_error', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=30, n_jobs=1,\n",
       "          param_distributions={'learning_rate': [0.1, 0.3, 0.5, 1], 'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1158fc5c0>, 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1158fc400>, 'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1158fc2e8>, 'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x115907048>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_squared_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsc_gbtr.fit(Train_x,Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50896.58408943678 {'learning_rate': 0.5, 'max_depth': 4, 'max_features': 8, 'min_samples_leaf': 5, 'n_estimators': 142}\n",
      "50288.3299513258 {'learning_rate': 0.3, 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 8, 'n_estimators': 139}\n",
      "72770.56255296334 {'learning_rate': 1, 'max_depth': 8, 'max_features': 3, 'min_samples_leaf': 6, 'n_estimators': 297}\n",
      "68654.39026826981 {'learning_rate': 1, 'max_depth': 6, 'max_features': 2, 'min_samples_leaf': 5, 'n_estimators': 200}\n",
      "65918.08962251447 {'learning_rate': 1, 'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 1, 'n_estimators': 98}\n",
      "50666.7897326738 {'learning_rate': 0.5, 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 4, 'n_estimators': 170}\n",
      "50995.20462696798 {'learning_rate': 0.1, 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 5, 'n_estimators': 206}\n",
      "49540.402704992186 {'learning_rate': 0.3, 'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 2, 'n_estimators': 92}\n",
      "50357.649971547165 {'learning_rate': 0.3, 'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 8, 'n_estimators': 74}\n",
      "66375.49843656334 {'learning_rate': 0.3, 'max_depth': 1, 'max_features': 4, 'min_samples_leaf': 2, 'n_estimators': 41}\n",
      "48645.93029753478 {'learning_rate': 0.3, 'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 4, 'n_estimators': 230}\n",
      "52540.43315366786 {'learning_rate': 0.3, 'max_depth': 2, 'max_features': 4, 'min_samples_leaf': 8, 'n_estimators': 254}\n",
      "68290.07610661557 {'learning_rate': 1, 'max_depth': 9, 'max_features': 8, 'min_samples_leaf': 5, 'n_estimators': 247}\n",
      "54488.321827547275 {'learning_rate': 0.1, 'max_depth': 2, 'max_features': 5, 'min_samples_leaf': 8, 'n_estimators': 256}\n",
      "72388.32503560983 {'learning_rate': 1, 'max_depth': 9, 'max_features': 1, 'min_samples_leaf': 9, 'n_estimators': 48}\n",
      "59322.909684147875 {'learning_rate': 1, 'max_depth': 1, 'max_features': 8, 'min_samples_leaf': 8, 'n_estimators': 102}\n",
      "54952.55817839994 {'learning_rate': 0.5, 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 8, 'n_estimators': 202}\n",
      "61531.09891547775 {'learning_rate': 0.5, 'max_depth': 1, 'max_features': 5, 'min_samples_leaf': 7, 'n_estimators': 80}\n",
      "63744.520655778615 {'learning_rate': 1, 'max_depth': 7, 'max_features': 9, 'min_samples_leaf': 8, 'n_estimators': 72}\n",
      "65973.09470564021 {'learning_rate': 1, 'max_depth': 7, 'max_features': 7, 'min_samples_leaf': 8, 'n_estimators': 138}\n",
      "71809.67604347762 {'learning_rate': 1, 'max_depth': 8, 'max_features': 6, 'min_samples_leaf': 3, 'n_estimators': 266}\n",
      "56103.62478832593 {'learning_rate': 0.1, 'max_depth': 3, 'max_features': 1, 'min_samples_leaf': 5, 'n_estimators': 257}\n",
      "52104.638119212694 {'learning_rate': 0.5, 'max_depth': 7, 'max_features': 9, 'min_samples_leaf': 3, 'n_estimators': 102}\n",
      "75044.3698516768 {'learning_rate': 1, 'max_depth': 7, 'max_features': 1, 'min_samples_leaf': 4, 'n_estimators': 135}\n",
      "63442.09833219819 {'learning_rate': 1, 'max_depth': 5, 'max_features': 7, 'min_samples_leaf': 7, 'n_estimators': 276}\n",
      "50094.76401635814 {'learning_rate': 0.5, 'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 3, 'n_estimators': 125}\n",
      "52974.019679568446 {'learning_rate': 1, 'max_depth': 2, 'max_features': 9, 'min_samples_leaf': 5, 'n_estimators': 173}\n",
      "59108.60027264933 {'learning_rate': 1, 'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 9, 'n_estimators': 270}\n",
      "63715.54692151498 {'learning_rate': 0.3, 'max_depth': 1, 'max_features': 1, 'min_samples_leaf': 9, 'n_estimators': 160}\n",
      "73682.73775431018 {'learning_rate': 1, 'max_depth': 9, 'max_features': 3, 'min_samples_leaf': 7, 'n_estimators': 237}\n"
     ]
    }
   ],
   "source": [
    "rscv = rsc_gbtr.cv_results_\n",
    "\n",
    "for  mean_score,params in zip(rscv['mean_test_score'],rscv['params']):\n",
    "    print(np.sqrt(-mean_score),params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prams_data = pd.DataFrame(list(rscv['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prams_data['Error']= np.sqrt(-rscv['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>230</td>\n",
       "      <td>48645.930298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>49540.402705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "      <td>50094.764016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>139</td>\n",
       "      <td>50288.329951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>74</td>\n",
       "      <td>50357.649972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>50666.789733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>142</td>\n",
       "      <td>50896.584089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>206</td>\n",
       "      <td>50995.204627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>52104.638119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>254</td>\n",
       "      <td>52540.433154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>173</td>\n",
       "      <td>52974.019680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>54488.321828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>202</td>\n",
       "      <td>54952.558178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>257</td>\n",
       "      <td>56103.624788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>270</td>\n",
       "      <td>59108.600273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>102</td>\n",
       "      <td>59322.909684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "      <td>61531.098915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>276</td>\n",
       "      <td>63442.098332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>160</td>\n",
       "      <td>63715.546922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>63744.520656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>65918.089623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>138</td>\n",
       "      <td>65973.094706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>66375.498437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>247</td>\n",
       "      <td>68290.076107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>68654.390268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>266</td>\n",
       "      <td>71809.676043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>72388.325036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>297</td>\n",
       "      <td>72770.562553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>237</td>\n",
       "      <td>73682.737754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>135</td>\n",
       "      <td>75044.369852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  max_depth  max_features  min_samples_leaf  n_estimators  \\\n",
       "10            0.3          6             6                 4           230   \n",
       "7             0.3          4             9                 2            92   \n",
       "25            0.5          4             7                 3           125   \n",
       "1             0.3          3             7                 8           139   \n",
       "8             0.3          4             7                 8            74   \n",
       "5             0.5          3             7                 4           170   \n",
       "0             0.5          4             8                 5           142   \n",
       "6             0.1          3             7                 5           206   \n",
       "22            0.5          7             9                 3           102   \n",
       "11            0.3          2             4                 8           254   \n",
       "26            1.0          2             9                 5           173   \n",
       "13            0.1          2             5                 8           256   \n",
       "16            0.5          3             1                 8           202   \n",
       "21            0.1          3             1                 5           257   \n",
       "27            1.0          4             7                 9           270   \n",
       "15            1.0          1             8                 8           102   \n",
       "17            0.5          1             5                 7            80   \n",
       "24            1.0          5             7                 7           276   \n",
       "28            0.3          1             1                 9           160   \n",
       "18            1.0          7             9                 8            72   \n",
       "4             1.0          6             9                 1            98   \n",
       "19            1.0          7             7                 8           138   \n",
       "9             0.3          1             4                 2            41   \n",
       "12            1.0          9             8                 5           247   \n",
       "3             1.0          6             2                 5           200   \n",
       "20            1.0          8             6                 3           266   \n",
       "14            1.0          9             1                 9            48   \n",
       "2             1.0          8             3                 6           297   \n",
       "29            1.0          9             3                 7           237   \n",
       "23            1.0          7             1                 4           135   \n",
       "\n",
       "           Error  \n",
       "10  48645.930298  \n",
       "7   49540.402705  \n",
       "25  50094.764016  \n",
       "1   50288.329951  \n",
       "8   50357.649972  \n",
       "5   50666.789733  \n",
       "0   50896.584089  \n",
       "6   50995.204627  \n",
       "22  52104.638119  \n",
       "11  52540.433154  \n",
       "26  52974.019680  \n",
       "13  54488.321828  \n",
       "16  54952.558178  \n",
       "21  56103.624788  \n",
       "27  59108.600273  \n",
       "15  59322.909684  \n",
       "17  61531.098915  \n",
       "24  63442.098332  \n",
       "28  63715.546922  \n",
       "18  63744.520656  \n",
       "4   65918.089623  \n",
       "19  65973.094706  \n",
       "9   66375.498437  \n",
       "12  68290.076107  \n",
       "3   68654.390268  \n",
       "20  71809.676043  \n",
       "14  72388.325036  \n",
       "2   72770.562553  \n",
       "29  73682.737754  \n",
       "23  75044.369852  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prams_data.sort_values('Error') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前两组表现较好的模型参数的急促上，在进行grid search方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = [{'max_depth':[4,6],\n",
    "         'max_features':[6,7,9],\n",
    "         'min_samples_leaf':[2,3,4],\n",
    "         'n_estimators':[100,200]}] #36种可能\n",
    "\n",
    "gscv_gbrt = GridSearchCV(GradientBoostingRegressor(learning_rate=0.3,random_state=42),params,cv=5, \n",
    "                          scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.3, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'max_depth': [4, 6], 'max_features': [6, 7, 9], 'min_samples_leaf': [2, 3, 4], 'n_estimators': [100, 200]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv_gbrt.fit(Train_x,Train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49968.36617931869 {'max_depth': 4, 'max_features': 6, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "48752.66224492643 {'max_depth': 4, 'max_features': 6, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "50025.44582826646 {'max_depth': 4, 'max_features': 6, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "49037.7090251143 {'max_depth': 4, 'max_features': 6, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "50208.23304943882 {'max_depth': 4, 'max_features': 6, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "48922.74446727734 {'max_depth': 4, 'max_features': 6, 'min_samples_leaf': 4, 'n_estimators': 200}\n",
      "49402.42256475303 {'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "48383.049078433374 {'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "49337.00503184393 {'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "48435.110273715014 {'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "49633.64746498155 {'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "48440.064911950954 {'max_depth': 4, 'max_features': 7, 'min_samples_leaf': 4, 'n_estimators': 200}\n",
      "49357.25569398467 {'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "48637.59039002851 {'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "49602.21535147241 {'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "48564.9991686678 {'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "49204.62842320357 {'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "48182.826664028325 {'max_depth': 4, 'max_features': 9, 'min_samples_leaf': 4, 'n_estimators': 200}\n",
      "48817.18275327822 {'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "48653.63425294729 {'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "48974.9158616112 {'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "48786.57358575641 {'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "48691.86218087695 {'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "48558.41536728501 {'max_depth': 6, 'max_features': 6, 'min_samples_leaf': 4, 'n_estimators': 200}\n",
      "48506.570073840296 {'max_depth': 6, 'max_features': 7, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "48482.17578684054 {'max_depth': 6, 'max_features': 7, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "48913.1822355383 {'max_depth': 6, 'max_features': 7, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "48615.36253553529 {'max_depth': 6, 'max_features': 7, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "48567.83125042734 {'max_depth': 6, 'max_features': 7, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "48443.47612528391 {'max_depth': 6, 'max_features': 7, 'min_samples_leaf': 4, 'n_estimators': 200}\n",
      "48852.91429693181 {'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "48862.91726722029 {'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 2, 'n_estimators': 200}\n",
      "48514.36869832902 {'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "48320.13722063705 {'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 3, 'n_estimators': 200}\n",
      "48016.11889854962 {'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "47939.63566240527 {'max_depth': 6, 'max_features': 9, 'min_samples_leaf': 4, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "gs_cv = gscv_gbrt.cv_results_\n",
    "for  mean_score,params in zip(gs_cv['mean_test_score'],gs_cv['params']):\n",
    "    print(np.sqrt(-mean_score),params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47939.63566240527,\n",
       " {'max_depth': 6,\n",
       "  'max_features': 9,\n",
       "  'min_samples_leaf': 4,\n",
       "  'n_estimators': 200})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(-gscv_gbrt.best_score_),gscv_gbrt.best_params_ #可以看到当我们选择以下参数的时候，获得了不到48000的误差，\n",
    "#是一个非常不错的表现了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 接下来如果我还要继续改进模型的话，该怎么做？一种方法是，用我们训练的模型对test_set对数据进行验证，判断模型在那一个区间或者说data space的预测效果较好，根据预测效果不好的data space进行进一步的探索（是数据量的问题？需要增加新的feature？还是说需要用新的模型来拟合这块预测表现不好的区域？然后也可以继续结合gradian boost的方法，对残差进行拟合，然后emsenble来获得最好的预测效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Emsemble方法的基本思想就是训练多个可能表现一般的模型，然后通过合并模型的方式来提升模型的预测表现，常用的方法包括bagging和boosting。bagging是通过集合在不完整的数据集上的模型来减少对训练集的‘记忆’，实践证明这种方法往往可以减少由variance带来的误差，其中随机森林在bagging的基础上，加入了对fearture的少量随机选取，能够更好地改善模型表现。Boosting方法，包括adaboost和gradiantboost，前者通过提升误差较大的预测值在loss function中的比重来来增强对误差数据的预测表现。后者则是通过连续地对误差地迭代训练，并合并多个模型来提升预测表现。可以看到不管是随机森林还是gradiant boost的表现都要远远地好于一般的线性模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
